<div align="center">
  <img src="https://raw.githubusercontent.com/open-compass/opencompass/main/docs/en/_static/image/logo.svg" height="100"/>
  <div>&nbsp;</div>
  <div align="center">
    <b><font size="5">OpenCompass Website</font></b>
    <sup>
      <a href="https://opencompass.org.cn/">
        <i><font size="4">HOT</font></i>
      </a>
    </sup>
    &nbsp;&nbsp;&nbsp;&nbsp;
    <b><font size="5">OpenCompass Toolkit</font></b>
    <sup>
      <a href="https://github.com/open-compass/OpenCompass/">
        <i><font size="4">TRY IT OUT</font></i>
      </a>
    </sup>
</div>
<div>&nbsp;</div>
</div>

![GitHub Org's stars](https://img.shields.io/github/stars/open-compass?style=social)

What is OpenCompass ?
OpenCompass is a platform focused on understanding of the AGI, include Large Language Model and Multi-modality Model. 

We aim to:

- develop high-quality libraries to reduce the difficulties in evaluation
- provide convincing leaderboards for improving the understanding of the large models
- create powerful toolchains targeting a variety of abilities and tasks
- build solid benchmarks to support the large model research
- research on inference of Large Model(analysis, reasoning, prompt engineering.)

## Toolkit

**OpenCompass**
- OpenCompass is an LLM evaluation platform, supporting a wide range of models (LLaMA, LLaMa2, ChatGLM2, ChatGPT, Claude, etc) over 80+ datasets.
- https://github.com/open-compass/opencompass

**VLMEvalKit**
- VLMEvalKit is a toolkit for evaluating large vision-language models (LVLMs), currently supporting ~20 LVLMs and five multi-modal benchmarks.
- https://github.com/open-compass/vlmevalkit

## Benchmarks and Methods


<table align="left">
  <tbody>
    <tr align="center" valign="bottom">
      <td>
        <b>Project</b>
      </td>
      <td>
        <b>Topic</b>
      </td> 
      <td>
        <b>Paper</b>
      </td> 
    </tr>
<tr valign="top">
  
<td>
  
  [DevBench](https://github.com/open-compass/DevBench)

</td>
<td>Automated Software Development</td>
<td>

[DevBench: Towards LLMs based Automated Software Development](https://arxiv.org/abs/2403.08604)

</td>
</tr>
<tr valign="top">
      
<td>
  
[CriticBench](https://github.com/open-compass/CriticBench)

</td>
       <td>Critic Reasoning</td>
      <td>

[CriticBench: Evaluating Large Language Models as Critic](https://arxiv.org/abs/2402.13764)

</td>
</tr>

<tr valign="top">      
<td>
  
[MathBench](https://github.com/open-compass/MathBench)

</td>
       <td>Mathematical Reasoning</td>
      <td>

[MathBench: Evaluating the Theory and Application Proficiency of LLMs with a Hierarchical Mathematics Benchmark](https://github.com/open-compass/MathBench)

</td>
</tr>
    <tr valign="top">
      <td>
  
[T-Eval](https://github.com/open-compass/T-Eval)

</td>
      <td>Tool Utilization</td>
      <td>

[T-Eval: Evaluating the Tool Utilization Capability Step by Step](https://arxiv.org/abs/2312.14033)

</td>

</tr>
      
<td>
  
[MMBench](https://github.com/open-compass/mmbench/)

</td>
       <td>Multi Modality</td>
      <td>

[MMBench: Is Your Multi-modal Model an All-around Player?](https://arxiv.org/abs/2307.06281)

</td>
</tr><tr valign="top">
  
<td>
  
[BotChat](https://github.com/open-compass/BotChat)

</td>
  <td>Subjective Evaluation</td>
      <td>

[BotChat: Evaluating LLMsâ€™ Capabilities of Having Multi-Turn Dialogues](https://arxiv.org/abs/2310.13650)

</td>
</tr>
</tr><tr valign="top">
  <td>
  
[LawBench](https://github.com/open-compass/LawBench/)

</td>
  <td>Domain Evaluation</td>
      <td>

[LawBench: Benchmarking Legal Knowledge of Large Language Models](https://arxiv.org/abs/2309.16289)

</td>

</tr>
  </tbody>
</table>

